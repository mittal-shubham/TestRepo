{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAOStsuMAvxGMC4QGm6MpL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mittal-shubham/TestRepo/blob/main/stock%20analysis%20assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance\n",
        "!pip install matplotlib\n",
        "!pip install html5lib\n",
        "!pip install lxml\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "url = \"http://www.ibm.com\"\n",
        "data  = requests.get(url).text\n",
        "soup = BeautifulSoup(data,\"html5lib\")\n",
        "\n",
        "for link in soup.find_all('a', href=True):\n",
        "    print(link.get('href'))\n",
        "\n",
        "for link in soup.find_all('img'):\n",
        "    print(link.get('src'))\n",
        "\n",
        "tables = soup.find_all('table')\n",
        "\n",
        "table_index = -1\n",
        "\n",
        "for index,table in enumerate(tables):\n",
        "    if (\"Annual Report\" in table.get_text()):\n",
        "        table_index = index\n",
        "        break\n",
        "\n",
        "if table_index != -1:\n",
        "  print(table_index)\n",
        "\n",
        "  financial_data = pd.read_html(str(tables[table_index]), flavor='bs4')[0]\n",
        "\n",
        "  financial_data.columns = financial_data.iloc[0]\n",
        "\n",
        "  financial_data = financial_data[1:]\n",
        "\n",
        "  financial_data\n",
        "else:\n",
        "  print(\"No table containing 'Annual Report' was found on the page.\")"
      ],
      "metadata": {
        "id": "MS4kJzeiItGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uIkyMIrpI7gN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "html_snippet = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "<title>Page Title</title>\n",
        "</head>\n",
        "<body>\n",
        "<h3><b id='boldest'>Lebron James</b></h3>\n",
        "<p> Salary: $ 92,000,000 </p>\n",
        "<h3> Stephen Curry</h3>\n",
        "<p> Salary: $85,000, 000 </p>\n",
        "<h3> Kevin Durant </h3>\n",
        "<p> Salary: $73,200, 000</p>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Use BeautifulSoup to parse the HTML snippet\n",
        "soup_snippet = BeautifulSoup(html_snippet, \"html5lib\")\n",
        "\n",
        "# Find all h3 tags\n",
        "h3_tags = soup_snippet.find_all('h3')\n",
        "\n",
        "# Find all p tags\n",
        "p_tags = soup_snippet.find_all('p')\n",
        "\n",
        "# Extract player names and salaries\n",
        "players = [h3.get_text().strip() for h3 in h3_tags]\n",
        "salaries = [p.get_text().replace('Salary:', '').strip() for p in p_tags]\n",
        "\n",
        "# Create a DataFrame\n",
        "data = {'Player': players, 'Salary': salaries}\n",
        "df_players = pd.DataFrame(data)\n",
        "\n",
        "df_players"
      ],
      "metadata": {
        "id": "Kc5s0EhTJ7is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"http://www.ibm.com\"\n",
        "data  = requests.get(url).text\n",
        "soup = BeautifulSoup(data,\"html5lib\")\n",
        "\n",
        "for link in soup.find_all('a', href=True):\n",
        "    print(link.get('href'))\n",
        "\n",
        "for link in soup.find_all('img'):\n",
        "    print(link.get('src'))\n",
        "\n",
        "tables = soup.find_all('table')\n",
        "\n",
        "table_index = -1 # Initialize table_index to -1\n",
        "\n",
        "for index,table in enumerate(tables):\n",
        "    if (\"Annual Report\" in table.get_text()):\n",
        "        table_index = index\n",
        "        break\n",
        "\n",
        "if table_index != -1:\n",
        "  print(table_index)\n",
        "\n",
        "  financial_data = pd.read_html(str(tables[table_index]), flavor='bs4')[0]\n",
        "\n",
        "  financial_data.columns = financial_data.iloc[0]\n",
        "\n",
        "  financial_data = financial_data[1:]\n",
        "\n",
        "  print(financial_data)\n",
        "else:\n",
        "  print(\"No table containing 'Annual Report' was found on the page.\")\n",
        "\n",
        "task = \"Extract data from a webpage using BeautifulSoup\"\n",
        "\n",
        "html_snippet = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "<title>Page Title</title>\n",
        "</head>\n",
        "<body>\n",
        "<h3><b id='boldest'>Lebron James</b></h3>\n",
        "<p> Salary: $ 92,000,000 </p>\n",
        "<h3> Stephen Curry</h3>\n",
        "<p> Salary: $85,000, 000 </p>\n",
        "<h3> Kevin Durant </h3>\n",
        "<p> Salary: $73,200, 000</p>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Use BeautifulSoup to parse the HTML snippet\n",
        "soup_snippet = BeautifulSoup(html_snippet, \"html5lib\")\n",
        "\n",
        "# Find all h3 tags\n",
        "h3_tags = soup_snippet.find_all('h3')\n",
        "\n",
        "# Find all p tags\n",
        "p_tags = soup_snippet.find_all('p')\n",
        "\n",
        "# Extract player names and salaries\n",
        "players = [h3.get_text().strip() for h3 in h3_tags]\n",
        "salaries = [p.get_text().replace('Salary:', '').strip() for p in p_tags]\n",
        "\n",
        "# Create a DataFrame\n",
        "data = {'Player': players, 'Salary': salaries}\n",
        "df_players = pd.DataFrame(data)\n",
        "\n",
        "df_players"
      ],
      "metadata": {
        "id": "BkVJ2uE0KRIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'boldest' is the element we want to find the siblings of\n",
        "boldest_element = soup_snippet.find(id='boldest')\n",
        "\n",
        "if boldest_element:\n",
        "  # Get the next sibling of the 'boldest' element\n",
        "  next_sibling = boldest_element.find_next_sibling()\n",
        "  if next_sibling:\n",
        "    print(\"Next sibling:\")\n",
        "    print(next_sibling)\n",
        "  else:\n",
        "    print(\"No next sibling found.\")\n",
        "\n",
        "  # Get the previous sibling of the 'boldest' element\n",
        "  previous_sibling = boldest_element.find_previous_sibling()\n",
        "  if previous_sibling:\n",
        "    print(\"Previous sibling:\")\n",
        "    print(previous_sibling)\n",
        "  else:\n",
        "    print(\"No previous sibling found.\")\n",
        "\n",
        "  # Get all siblings (excluding itself) by finding the parent and then siblings\n",
        "  parent_element = boldest_element.parent\n",
        "  if parent_element:\n",
        "    all_siblings = parent_element.find_all(lambda tag: tag != boldest_element and tag.parent == parent_element)\n",
        "    if all_siblings:\n",
        "      print(\"All siblings:\")\n",
        "      for sibling in all_siblings:\n",
        "        print(sibling)\n",
        "    else:\n",
        "      print(\"No siblings found.\")\n",
        "  else:\n",
        "    print(\"Parent element not found.\")\n",
        "\n",
        "else:\n",
        "  print(\"Element with id='boldest' not found.\")"
      ],
      "metadata": {
        "id": "J3w4gH4PLIdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "html_table = \"\"\"\n",
        "<table>\n",
        "  <tr>\n",
        "    <td id='flight' >Flight No</td>\n",
        "    <td>Launch site</td>\n",
        "    <td>Payload mass</td>\n",
        "   </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a></td>\n",
        "    <td>300 kg</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2</td>\n",
        "    <td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td>\n",
        "    <td>94 kg</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3</td>\n",
        "    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a> </td>\n",
        "    <td>80 kg</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\"\"\"\n",
        "\n",
        "# Parse the HTML\n",
        "soup_table = BeautifulSoup(html_table, 'html5lib')\n",
        "\n",
        "# Find all <td> elements\n",
        "td_elements = soup_table.find_all('td')\n",
        "\n",
        "# Iterate through the <td> elements and check if they contain an 'a' tag without an href\n",
        "for td in td_elements:\n",
        "    a_tag = td.find('a')\n",
        "    if a_tag is None or 'href' not in a_tag.attrs:\n",
        "        print(td)"
      ],
      "metadata": {
        "id": "mpKYMJ3NMdj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "boldest_element = soup_snippet.find(id=\"boldest\")\n",
        "boldest_element\n"
      ],
      "metadata": {
        "id": "sOJpmSs5NB_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<h3>Rocket Launch </h3>\n",
        "<p>\n",
        "<table class='rocket'>\n",
        "  <tr>\n",
        "    <td>Flight No</td>\n",
        "    <td>Launch site</td>\n",
        "    <td>Payload mass</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Florida</td>\n",
        "    <td>300 kg</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2</td>\n",
        "    <td>Texas</td>\n",
        "    <td>94 kg</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3</td>\n",
        "    <td>Florida </td>\n",
        "    <td>80 kg</td>\n",
        "  </tr>\n",
        "</table>\n",
        "</p>\n",
        "<p>\n",
        "<h3>Pizza Party  </h3>\n",
        "<table class='pizza'>\n",
        "  <tr>\n",
        "    <td>Pizza Place</td>\n",
        "    <td>Orders</td>\n",
        "    <td>Slices </td>\n",
        "   </tr>\n",
        "  <tr>\n",
        "    <td>Domino's Pizza</td>\n",
        "    <td>10</td>\n",
        "    <td>100</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Little Caesars</td>\n",
        "    <td>12</td>\n",
        "    <td >144 </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Papa John's </td>\n",
        "    <td>15 </td>\n",
        "    <td>165</td>\n",
        "  </tr>\n",
        "</table>\n",
        "</p>"
      ],
      "metadata": {
        "id": "G-HRsWGBNQlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://web.archive.org/web/20230224123642/https://www.ibm.com/us-en/\"\n",
        "data  = requests.get(url).text\n",
        "soup = BeautifulSoup(data,\"html5lib\")\n",
        "\n",
        "for link in soup.find_all('a', href=True):\n",
        "    print(link.get('href'))"
      ],
      "metadata": {
        "id": "sgqQiXRrNibm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for link in soup.find_all('img'):\n",
        "    print(link)\n",
        "    print(link.get('src'))\n"
      ],
      "metadata": {
        "id": "Pp4kCnJKNqkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/labs/datasets/HTMLColorCodes.html\"\n",
        "data  = requests.get(url).text\n",
        "soup = BeautifulSoup(data,\"html5lib\")\n",
        "\n",
        "# Find all tables in the parsed HTML\n",
        "tables = soup.find_all('table')\n",
        "\n",
        "# Assuming there is only one table on the page based on the URL\n",
        "# Read the first table found into a pandas DataFrame\n",
        "df = pd.read_html(str(tables[0]), flavor='bs4')[0]\n",
        "\n",
        "df\n"
      ],
      "metadata": {
        "id": "CBIfgQV3NxS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://en.wikipedia.org/wiki/World_population\"\n",
        "data = requests.get(url).text\n",
        "soup = BeautifulSoup(data, \"html5lib\")\n",
        "tables = pd.read_html(str(soup.find_all('table')))\n",
        "tables = tables[6] # Selecting the relevant table, adjust index if needed\n",
        "tables.columns = tables.iloc[0]\n",
        "tables = tables[1:]\n",
        "tables"
      ],
      "metadata": {
        "id": "D5bJeuBsN-Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tables)"
      ],
      "metadata": {
        "id": "1DdOt7BePNOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "amd = yf.Ticker(\"AMD\")\n",
        "amd\n"
      ],
      "metadata": {
        "id": "zWzsEqiJRR7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/data/amd.json"
      ],
      "metadata": {
        "id": "km0uZGbpRWKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "amd_info = amd.info\n",
        "print(\"Country:\", amd_info['country'])\n",
        "print(\"Sector:\", amd_info['sector'])"
      ],
      "metadata": {
        "id": "heZJ5OClRlZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "amd_share_price_data = amd.history(period=\"max\")\n",
        "print(amd_share_price_data.iloc[0]['Volume'])"
      ],
      "metadata": {
        "id": "_a6-sAgZRzbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ],
      "metadata": {
        "id": "d7axBEReSQW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netflix_data = pd.DataFrame(columns=[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"])\n",
        "netflix_data"
      ],
      "metadata": {
        "id": "UEEj6yIfSxLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/amazon_data_webpage.html\"\n",
        "html_data = requests.get(url).text"
      ],
      "metadata": {
        "id": "JoKf-PvrUYfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(html_data, \"html5lib\")\n",
        "\n",
        "# Find all tables in the parsed HTML\n",
        "amazon_tables = soup.find_all('table')\n",
        "\n",
        "# Assuming the Amazon stock data table is the first table on the page\n",
        "amazon_data = pd.read_html(str(amazon_tables[0]), flavor='bs4')[0]\n",
        "\n",
        "# Display the first 5 rows of the DataFrame\n",
        "print(amazon_data.head(5))"
      ],
      "metadata": {
        "id": "9h8mqxNLUs-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(soup_snippet.title.get_text())"
      ],
      "metadata": {
        "id": "ugZUUqhNU4K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/amazon_data_webpage.html\"\n",
        "html_data = requests.get(url).text\n",
        "\n",
        "soup = BeautifulSoup(html_data, \"html5lib\")\n",
        "\n",
        "amazon_data = pd.DataFrame(columns=[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"])\n",
        "\n",
        "# Create a list to store the rows\n",
        "data_rows = []\n",
        "\n",
        "for row in soup.find(\"tbody\").find_all(\"tr\"):\n",
        "  col = row.find_all(\"td\")\n",
        "  date = col[0].text\n",
        "  Open = col[1].text\n",
        "  high = col[2].text\n",
        "  low = col[3].text\n",
        "  close = col[4].text\n",
        "  adj_close = col[5].text\n",
        "  volume = col[6].text\n",
        "\n",
        "  # Append the row as a dictionary to the list\n",
        "  data_rows.append({\"Date\": date, \"Open\": Open, \"High\": high, \"Low\": low, \"Close\": close, \"Adj Close\": adj_close, \"Volume\": volume})\n",
        "\n",
        "# Concatenate the list of dictionaries to the DataFrame\n",
        "amazon_data = pd.concat([amazon_data, pd.DataFrame(data_rows)], ignore_index=True)\n",
        "\n",
        "print(amazon_data.head())"
      ],
      "metadata": {
        "id": "0wcZaHZHVEu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "amazon_data.columns"
      ],
      "metadata": {
        "id": "dfHTdUnxVPzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(amazon_data.iloc[-1]['Open'])"
      ],
      "metadata": {
        "id": "21D-mQhtVVmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tesla = yf.Ticker(\"TSLA\")\n",
        "tesla_data = tesla.history(period=\"max\")\n",
        "\n",
        "tesla_data.reset_index(inplace=True)\n",
        "tesla_data.head()"
      ],
      "metadata": {
        "id": "6DojEbR-7qih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/revenue.htm\"\n",
        "html_data = requests.get(url).text\n",
        "\n",
        "soup = BeautifulSoup(html_data, \"html5lib\")\n",
        "\n",
        "# Find all tables on the page\n",
        "tables = soup.find_all('table')\n",
        "\n",
        "# Assuming the Tesla revenue table is the second table on the page (index 1)\n",
        "# Inspect the HTML to confirm the correct table index\n",
        "tesla_revenue = pd.read_html(str(tables[1]), flavor='bs4')[0]\n",
        "\n",
        "# Rename columns to be more descriptive if needed\n",
        "tesla_revenue.columns = ['Date', 'Revenue']\n",
        "\n",
        "# Display the last five rows of the tesla_revenue dataframe\n",
        "print(tesla_revenue.tail())"
      ],
      "metadata": {
        "id": "qtQbv0Fl8FUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gme = yf.Ticker(\"GME\")\n",
        "gme_data = gme.history(period=\"max\")\n",
        "\n",
        "gme_data.reset_index(inplace=True)\n",
        "gme_data.head()"
      ],
      "metadata": {
        "id": "htPFWamu8iej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/stock.html\"\n",
        "html_data = requests.get(url).text\n",
        "\n",
        "soup = BeautifulSoup(html_data, \"html5lib\")\n",
        "\n",
        "gme_tables = soup.find_all('table')\n",
        "\n",
        "gme_revenue = pd.read_html(str(gme_tables[1]), flavor='bs4')[0]\n",
        "\n",
        "gme_revenue.columns = ['Date', 'Revenue']\n",
        "\n",
        "# Display the last five rows of the gme_revenue dataframe\n",
        "print(gme_revenue.tail())\n"
      ],
      "metadata": {
        "id": "nymyO7gC9iEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_graph(stock_data, stock):\n",
        "    # Ensure 'Date' is in datetime format\n",
        "    stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
        "\n",
        "    # Filter for data after 2017\n",
        "    stock_data_filtered = stock_data[stock_data['Date'].dt.year >= 2017]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot stock data Close price\n",
        "    plt.plot(stock_data_filtered['Date'], stock_data_filtered['Close'], label=f'{stock} Stock Price', color='blue')\n",
        "\n",
        "    plt.title(f'{stock} Stock Price and Revenue Over Time (Post 2017)')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "make_graph(tesla_data,'Tesla')"
      ],
      "metadata": {
        "id": "W7QnrFHDAkQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_graph(gme_data, 'GameStop')"
      ],
      "metadata": {
        "id": "f0xLokopBNkn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
